{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad9f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages that we need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import io\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2189bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = r'C:\\Users\\Reinaldo\\Documents\\10 data + model george-20220531T141603Z-001\\10 data + model george\\img_all_weight_15\\img_all_weight_15'\n",
    "CATEGORIES = os.listdir(r\"C:\\Users\\Reinaldo\\Documents\\10 data + model george-20220531T141603Z-001\\10 data + model george\\img_all_weight_15\\img_all_weight_15\")\n",
    "#CATEGORIES = ['Abutilon Hybridum','Alpinia Galanga (Rasna)', 'Amaranthus Viridis (Arive-Dantu)', 'Anthurium Scherzerianum', 'Aporocactus Flagelliformis', 'Basella Alba (Basale)', 'Calathea Crocata', 'daisy', 'dandelion', 'Ficus Religiosa (Peepal Tree)', 'Hibiscus rosa-sinensis', 'Lithops fulleri', 'Lithops optica _Rubra_', 'Lithops pseudotruncatella', 'Moringa Oleifera (Drumstick)', 'Nerium Oleander (Oleander)', 'Ocimum Tenuiflorum (Tulsi)', 'Pachystachys Lutea', 'Paphiopedilum Venustum', 'Peperomia Argyela', 'Phalaenopsis amabilis', 'Plectranthus Amboinicus (Mexican Mint)', 'Punica Granatum (Pomegranate)', 'rose', 'Schlumbergera Bridgesii', 'Strelitzia Reginae', 'sunflower', 'Trigonella Foenum-graecum (Fenugreek)', 'tulip', 'Asplenium Nidus', 'Calathea Roseopicta', 'Calathea Zebrina', 'Chlorophytum Comosum', 'Davallia Fejeensis', 'Ficus Auriculata (Roxburgh fig)', 'Mentha (Mint)', 'Nephrolepis Exaltata', 'Opuntia Microdasys', 'Pilea Cadierei', 'Piper Betle (Betel)', 'Santalum Album (Sandalwood)', 'Sedum Morganianum', 'Monstera Deliciosa', 'snow white aglaonema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83347fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = DATADIR\n",
    "\n",
    "# source_path_dogs = os.path.join(source_path, 'Anthurium Scherzerianum')\n",
    "# source_path_cats = os.path.join(source_path, 'daisy')\n",
    "\n",
    "\n",
    "# # os.listdir returns a list containing all files under the given path\n",
    "# print(f\"There are {len(os.listdir(source_path_dogs))} images of Aporocactus Flagelliformis.\")\n",
    "# print(f\"There are {len(os.listdir(source_path_cats))} images of Schlumbergera gaertneri.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a131e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root directoryeifera (Drumstick)', 'Nerium Oleander (Oleander)', 'Ocimum Tenuiflorum (Tulsi)', 'Pachystachys Lutea', 'Paphiopedilum Venustum', 'Peperomia Argyela', 'Phalaenopsis amabilis', 'Plectranthus Amboinicus (Mexican Mint)', 'Punica Granatum (Pomegranate)', 'rose', 'Schlumbergera Bridgesii', 'Strelitzia Reginae', 'sunflower', 'Trigonella Foenum-graecum (Fenugreek)', 'tulip', 'Asplenium Nidus', 'Calathea Roseopicta', 'Calathea Zebrina', 'Chlorophytum Comosum', 'Davallia Fejeensis', 'Ficus Auriculata (Roxburgh fig)', 'Mentha (Mint)', 'Nephrolepis Exaltata', 'Opuntia Microdasys', 'Pilea Cadierei', 'Piper Betle (Betel)', 'Santalum Album (Sandalwood)', 'Sedum Morganianum', 'Monstera Deliciosa', 'snow white aglaonema']\n",
    "root_dir = r'C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama'\n",
    "\n",
    "# Empty directory to prevent FileExistsError is the function is run several times\n",
    "if os.path.exists(root_dir):\n",
    "  shutil.rmtree(root_dir)\n",
    "\n",
    "# GRADED FUNCTION: create_train_test_dirs\n",
    "def create_train_test_dirs(root_path):\n",
    "\n",
    "  # Use os.makedirs to create your directories with intermediate subdirectories\n",
    "  # Don't hardcode the paths. Use os.path.join to append the new directories to the root_path parameter\n",
    "  train_dir = os.path.join(root_path, 'training')\n",
    "  os.makedirs(train_dir)\n",
    "  validation_dir = os.path.join(root_path, 'validation')\n",
    "  os.makedirs(validation_dir)\n",
    "  test_dir = os.path.join(root_path, 'testing')\n",
    "  os.makedirs(test_dir)\n",
    "  # Directory with training cat/dog pictures\n",
    "  for tanaman in CATEGORIES:\n",
    "        os.makedirs(os.path.join(train_dir, tanaman))\n",
    "\n",
    "  # Directory with validation cat/dog pictures\n",
    "  for tanaman in CATEGORIES:\n",
    "        os.makedirs(os.path.join(validation_dir, tanaman))\n",
    "  for tanaman in CATEGORIES:\n",
    "        os.makedirs(os.path.join(test_dir, tanaman))\n",
    "\n",
    "\n",
    "  \n",
    "try:\n",
    "  create_train_test_dirs(root_path=root_dir)\n",
    "except FileExistsError:\n",
    "  print(\"You should not be seeing this since the upper directory is removed beforehand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217dd00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\\17_Soleirolia soleirolii\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\\20_Gerbera Daisy\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\\26_Nymphaea\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\\29_Iris  plant\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\\30_Lavandula plant\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\\Chinese Elm Bonsai\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\\Haworthiopsis fasciata\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\\Hypoestes phyllostachya\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\\Kalanchoe thyrsiflora\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\\Lithops\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\\17_Soleirolia soleirolii\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\\20_Gerbera Daisy\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\\26_Nymphaea\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\\29_Iris  plant\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\\30_Lavandula plant\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\\Chinese Elm Bonsai\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\\Haworthiopsis fasciata\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\\Hypoestes phyllostachya\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\\Kalanchoe thyrsiflora\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\\Lithops\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\\17_Soleirolia soleirolii\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\\20_Gerbera Daisy\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\\26_Nymphaea\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\\29_Iris  plant\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\\30_Lavandula plant\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\\Chinese Elm Bonsai\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\\Haworthiopsis fasciata\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\\Hypoestes phyllostachya\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\\Kalanchoe thyrsiflora\n",
      "C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\\Lithops\n"
     ]
    }
   ],
   "source": [
    "# Test your create_train_test_dirs function\n",
    "\n",
    "for rootdir, dirs, files in os.walk(root_dir):\n",
    "    for subdir in dirs:\n",
    "        print(os.path.join(rootdir, subdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "588cbf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: split_data\n",
    "def split_data(SOURCE, TRAINING, VALIDATION, TESTING, SPLIT_SIZE1, SPLIT_SIZE2):\n",
    "\n",
    "  ### START CODE HERE\n",
    "  #os.path.getsize(PATH)\n",
    "    list_name = os.listdir(SOURCE)\n",
    "    random.sample(list_name, len(list_name))\n",
    "  \n",
    "\n",
    "\n",
    "    part1 = int(len(list_name) * SPLIT_SIZE1)\n",
    "    training_file = list_name[:part1]\n",
    "    PREtesting_file =list_name[part1:]\n",
    "    part2 = int(len(PREtesting_file) * SPLIT_SIZE2)\n",
    "    validation_file = PREtesting_file[:part2]\n",
    "    testing_file =PREtesting_file[part2:]\n",
    "\n",
    "    for file in training_file:\n",
    "        copyfile(os.path.join(SOURCE, file), os.path.join(TRAINING, file))\n",
    "    for file in validation_file:\n",
    "        copyfile(os.path.join(SOURCE, file), os.path.join(VALIDATION, file))\n",
    "    for file in testing_file:\n",
    "        copyfile(os.path.join(SOURCE, file), os.path.join(TESTING, file))\n",
    "#      copyfile(SOURCE+file, TESTING+file)\n",
    "\n",
    "  ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a55be7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = r\"C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\training\"\n",
    "VALIDATION_DIR = r\"C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\validation\"\n",
    "TESTING_DIR = r\"C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanUtama\\testing\"\n",
    "\n",
    "for tanaman in CATEGORIES:\n",
    "    if len(os.listdir(os.path.join(TRAINING_DIR, tanaman))) > 0:\n",
    "        for file in os.scandir(os.path.join(TRAINING_DIR, tanaman)):\n",
    "            os.remove(file.path)\n",
    "    if len(os.listdir(os.path.join(VALIDATION_DIR, tanaman))) > 0:\n",
    "        for file in os.scandir(os.path.join(VALIDATION_DIR, tanaman)):\n",
    "            os.remove(file.path)\n",
    "    if len(os.listdir(os.path.join(TESTING_DIR, tanaman))) > 0:\n",
    "        for file in os.scandir(os.path.join(TESTING_DIR, tanaman)):\n",
    "            os.remove(file.path)\n",
    "\n",
    "# Define proportion of images used for training\n",
    "split_size1 = .9\n",
    "split_size2 = 1\n",
    "# Run the function\n",
    "# NOTE: Messages about zero length images should be printed out\n",
    "for tanaman in CATEGORIES:\n",
    "    split_data(os.path.join(source_path, tanaman), os.path.join(TRAINING_DIR, tanaman), os.path.join(VALIDATION_DIR, tanaman), os.path.join(TESTING_DIR, tanaman), split_size1, split_size2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cde7d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_val_generators\n",
    "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
    "  ### START CODE HERE\n",
    "\n",
    "  # Instantiate the ImageDataGenerator class (don't forget to set the arguments to augment the images)\n",
    "  train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                     rotation_range=20,\n",
    "                                     width_shift_range=.2,\n",
    "                                     height_shift_range=.2,\n",
    "#                                      shear_range=0.2,\n",
    "                                     zoom_range=0.2,\n",
    "                                     horizontal_flip=True,\n",
    "                                     fill_mode=\"nearest\")\n",
    "\n",
    "  # Pass in the appropriate arguments to the flow_from_directory method\n",
    "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
    "                                                      batch_size=32,\n",
    "                                                      class_mode='categorical',\n",
    "                                                      target_size=(224, 224), color_mode='rgb')\n",
    "\n",
    "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
    "  validation_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
    "\n",
    "  # Pass in the appropiate arguments to the flow_from_directory method\n",
    "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
    "                                                                batch_size=32,\n",
    "                                                                class_mode='categorical',\n",
    "                                                                target_size=(224, 224),\n",
    "                                                                color_mode='rgb')\n",
    "  ### END CODE HERE\n",
    "  return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f9f7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_segment_function(img_array):\n",
    "    img_array= np.rint(img_array)\n",
    "    img_array= img_array.astype('uint8')\n",
    "    hsv_img= cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n",
    "    mask = cv2.inRange(hsv_img, (24, 50, 0), (55, 255, 255))\n",
    "    result = cv2.bitwise_and(img_array, img_array, mask=mask)\n",
    "    result= result.astype('float64')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24ef8cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 794 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=color_segment_function)\n",
    "test_generator = test_datagen.flow_from_directory(r'C:\\Users\\Reinaldo\\Documents\\Bangkit\\DatasetTanamanlah\\testing',\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  batch_size=1,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbaf1a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5700 images belonging to 10 classes.\n",
      "Found 638 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Test your generators\n",
    "train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4843a383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['17_Soleirolia soleirolii',\n",
       " '20_Gerbera Daisy',\n",
       " '26_Nymphaea',\n",
       " '29_Iris  plant',\n",
       " '30_Lavandula plant',\n",
       " 'Chinese Elm Bonsai',\n",
       " 'Haworthiopsis fasciata',\n",
       " 'Hypoestes phyllostachya',\n",
       " 'Kalanchoe thyrsiflora',\n",
       " 'Lithops']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = list(train_generator.class_indices.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e09c3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------+--------------+\n",
      "| class_index |       class_label        | class_weight |\n",
      "+-------------+--------------------------+--------------+\n",
      "|      0      | 17_Soleirolia soleirolii |     1.09     |\n",
      "|      1      |     20_Gerbera Daisy     |     1.02     |\n",
      "|      2      |       26_Nymphaea        |     0.81     |\n",
      "|      3      |      29_Iris  plant      |     0.85     |\n",
      "|      4      |    30_Lavandula plant    |     0.84     |\n",
      "|      5      |    Chinese Elm Bonsai    |     1.28     |\n",
      "|      6      |  Haworthiopsis fasciata  |     1.29     |\n",
      "|      7      | Hypoestes phyllostachya  |     0.87     |\n",
      "|      8      |  Kalanchoe thyrsiflora   |     0.94     |\n",
      "|      9      |         Lithops          |     1.00     |\n",
      "+-------------+--------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "#get class indices and labels. calculate class weight\n",
    "label_map = {}\n",
    "for k, v in train_generator.class_indices.items():\n",
    "    label_map[v]=k\n",
    "\n",
    "class_counts= pd.Series(train_generator.classes).value_counts()\n",
    "class_weight= {}\n",
    "\n",
    "for i, c in class_counts.items():\n",
    "    class_weight[i]= 1.0/c\n",
    "    \n",
    "norm_factor= np.mean(list(class_weight.values()))\n",
    "\n",
    "for k in class_counts.keys():\n",
    "    class_weight[k]= class_weight[k]/norm_factor\n",
    "\n",
    "t = PrettyTable(['class_index', 'class_label', 'class_weight'])\n",
    "for i in sorted(class_weight.keys()):\n",
    "    t.add_row([i, label_map[i], '{:.2f}'.format(class_weight[i])])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc365b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_shape (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_224 (Funct  (None, None, None, 1280)  2257984  \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling_laye  (None, 1280)             0         \n",
      " r (GlobalAveragePooling2D)                                      \n",
      "                                                                 \n",
      " outputs (Dense)             (None, 10)                12810     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,270,794\n",
      "Trainable params: 12,810\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(include_top=False)\n",
    "base_model.trainable= False\n",
    "\n",
    "inputs = layers.Input(shape=(224,224,3),name = \"input_shape\",dtype=tf.float16)\n",
    "x = base_model(inputs,training=False)\n",
    "x = layers.GlobalAvgPool2D(name=\"global_average_pooling_layer\")(x)\n",
    "outputs = layers.Dense(len(class_names),activation='softmax',dtype=tf.float32,name='outputs')(x)\n",
    "\n",
    "model_2 = Model(inputs,outputs)\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6adf56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers\n",
    "# model =  tf.keras.models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), input_shape=(224, 224, 3),padding='Same', activation='relu'))\n",
    "# model.add(layers.MaxPool2D(pool_size=2 , strides=2))\n",
    "# model.add(layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3),padding='Same' , activation='relu'))\n",
    "# model.add(layers.MaxPool2D(pool_size=2 , strides=2))\n",
    "# model.add(layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3),padding='Same', activation='relu'))\n",
    "# model.add(layers.MaxPool2D(pool_size=2 , strides=2))\n",
    "# #model.add(layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3),padding='Same', activation='relu'))\n",
    "# model.add(layers.MaxPool2D(pool_size=2 , strides=2))\n",
    "# #model.add(layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dropout(0.4))\n",
    "# model.add(layers.Dense(256, activation='relu'))\n",
    "\n",
    "# model.add(layers.Dense(len(CATEGORIES), activation='softmax'))\n",
    "\n",
    "\n",
    "# # base_model = tf.keras.applications.MobileNetV2(include_top=False)\n",
    "# # base_model.trainable= False\n",
    "\n",
    "# # inputs = layers.Input(shape=(224,224,3),name = \"input_shape\",dtype=tf.float16)\n",
    "# # x = base_model(inputs,training=False)\n",
    "# # x = layers.GlobalAvgPool2D(name=\"global_average_pooling_layer\")(x)\n",
    "# # #x = layers.Dense(512, activation='relu')(x)\n",
    "# # outputs = layers.Dense(len(class_names),activation='softmax',dtype=tf.float32,name='outputs')(x)\n",
    "\n",
    "# # model_2 = Model(inputs,outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1071ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2.compile(optimizer=keras.optimizers.Adam(lr=0.001, amsgrad=True),\n",
    "#                     loss='categorical_crossentropy',\n",
    "#                     metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fc974a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_shape (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_224 (Funct  (None, None, None, 1280)  2257984  \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling_laye  (None, 1280)             0         \n",
      " r (GlobalAveragePooling2D)                                      \n",
      "                                                                 \n",
      " outputs (Dense)             (None, 10)                12810     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,270,794\n",
      "Trainable params: 12,810\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea94d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opsi callBack1\n",
    "\n",
    "# class MyThresholdCallback(tf.keras.callbacks.Callback):\n",
    "#     def __init__(self, threshold):\n",
    "#         super(MyThresholdCallback, self).__init__()\n",
    "#         self.threshold = threshold\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None): \n",
    "#         val_acc = logs[\"val_loss\"]\n",
    "#         if val_acc <= self.threshold:\n",
    "#             self.model.stop_training = True\n",
    "\n",
    "# # callback_model2 = ModelCheckpoint(\"checkpoint_all/model2_45Data_trad.h5\")\n",
    "\n",
    "# my_callback = MyThresholdCallback(threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb9ff6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
    "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
    "                                                 patience=2,\n",
    "                                                 verbose=1, # print out when learning rate goes down \n",
    "                                                 min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a3c6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "  if epoch <= 50:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.2)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0bed5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chckp = tf.keras.callbacks.ModelCheckpoint(\"./model_best.h5\", monitor='val_accuracy', mode='max', save_best_only=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce432729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opsi callBack2\n",
    "# best_cb= callbacks.ModelCheckpoint('model_best29Mei.h5', \n",
    "#                                          monitor='val_loss', \n",
    "#                                          verbose=1, \n",
    "#                                          save_best_only=True, \n",
    "#                                          save_weights_only=False, \n",
    "#                                          mode='auto', \n",
    "#                                          period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6708f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "179/179 [==============================] - 97s 526ms/step - loss: 0.8533 - accuracy: 0.7214 - val_loss: 0.4816 - val_accuracy: 0.8542 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "179/179 [==============================] - 84s 470ms/step - loss: 0.4506 - accuracy: 0.8560 - val_loss: 0.6535 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "179/179 [==============================] - 87s 487ms/step - loss: 0.3685 - accuracy: 0.8863 - val_loss: 0.2863 - val_accuracy: 0.8958 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "179/179 [==============================] - 87s 484ms/step - loss: 0.3333 - accuracy: 0.8935 - val_loss: 0.4103 - val_accuracy: 0.8438 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "179/179 [==============================] - 84s 471ms/step - loss: 0.3057 - accuracy: 0.9028 - val_loss: 0.3070 - val_accuracy: 0.9167 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history_model2 = model_2.fit(train_generator,\n",
    "                                 epochs=5,\n",
    "                                 steps_per_epoch=len(train_generator),\n",
    "                                 validation_data=validation_generator,\n",
    "                                 validation_steps=int(0.15 * len(validation_generator)), \n",
    "                                 callbacks=[lr_callback, chckp],\n",
    "                             class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56bb1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# epochs = 150\n",
    "# steps_per_epoch = 50\n",
    "# # Train the model\n",
    "# # Note that this may take some time.\n",
    "# history = model_2.fit(train_generator,\n",
    "#                              class_weight= class_weight,\n",
    "#                              epochs=20,\n",
    "#                              steps_per_epoch=steps_per_epoch,\n",
    "#                              #steps_per_epoch=len(train_generator),\n",
    "#                              validation_data=validation_generator,\n",
    "#                              #validation_steps=int(0.15 * len(validation_generator)), \n",
    "#                              callbacks=[my_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history_model2.history['accuracy']\n",
    "val_acc=history_model2.history['val_accuracy']\n",
    "loss=history_model2.history['loss']\n",
    "val_loss=history_model2.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "#------------------------------------------------\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5cbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_model_maker import image_classifier\n",
    "from tflite_model_maker.image_classifier import DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "model_2.export(export_dir=r'C:\\Users\\Reinaldo\\Documents', tflite_filename='MedicineLeaf.tflite', quantization_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save('model_best_17class.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8084044",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model_2, r'C:\\Users\\Reinaldo\\Documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save_weights(r'C:\\Users\\Reinaldo\\Documents',save_format = 'h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb669779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers, callbacks\n",
    "model= models.load_model('model_best_17class.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26ec0146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Reinaldo\\Documents\\Model-17Class\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Reinaldo\\Documents\\Model-17Class\\assets\n"
     ]
    }
   ],
   "source": [
    "model_save_path = r'C:\\Users\\Reinaldo\\Documents\\Model-17Class'\n",
    "tf.saved_model.save(model_2, model_save_path)\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_save_path) # path to the SavedModel directory\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.allow_custom_ops=True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "os.makedirs('tflite_models', exist_ok=True)\n",
    "with open(r'C:\\Users\\Reinaldo\\Documents\\Model-17Class\\tflite_models\\cobaa3_types.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b048e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "#converter.experimental_new_converter = True\n",
    "tfmodel = converter.convert()\n",
    "open('yourmodel.tflite', 'wb').write(tfmodel) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737339c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tflite-model-maker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be488c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_model_maker import image_classifier\n",
    "from tflite_model_maker.image_classifier import DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "\n",
    "config = QuantizationConfig.for_float16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "pred= model.predict_generator(test_generator, steps= test_generator.n, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(pred,axis=1)\n",
    "\n",
    "prediction_labels = [label_map[k] for k in predicted_class_indices]\n",
    "filenames= test_generator.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile= open('ray_reed_submission.csv', 'w', newline='')\n",
    "writer= csv.writer(csvfile)\n",
    "\n",
    "headers= ['file', 'species']\n",
    "\n",
    "writer.writerow(headers)\n",
    "t = PrettyTable(headers)\n",
    "for i, f, p in zip(range(len(filenames)), filenames, prediction_labels):\n",
    "    writer.writerow([os.path.basename(f),p])\n",
    "    if i <10:\n",
    "        t.add_row([os.path.basename(f), p])\n",
    "    elif i<13:\n",
    "        t.add_row(['.', '.'])\n",
    "csvfile.close()\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4be6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_prep_predict(path):\n",
    "    img = plt.imread(path)\n",
    "    img=tf.image.resize(img,(224,224))\n",
    "    img=tf.expand_dims(img,axis=0)\n",
    "    img = img/255.\n",
    "    predmod = np.ravel(model_2.predict(img))\n",
    "    a= 0\n",
    "    nji = 0\n",
    "    for CATEGORIE in CATEGORIES:\n",
    "        if predmod[a] > nji:\n",
    "            mosnt = CATEGORIE\n",
    "            nji = predmod[a]\n",
    "        a+=1\n",
    "\n",
    "    return mosnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.evaluate(validation_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
